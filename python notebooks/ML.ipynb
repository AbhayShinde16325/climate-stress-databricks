{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc554fe-ae04-4e11-9de4-3f13ddf39eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_country_df = spark.table(\"gold_climate_country_indicators\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef03730-d2af-46cf-afec-07c3da3d62a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ml_df = gold_country_df.select(\n",
    "    \"year\",\n",
    "    \"avg_yearly_temperature\",\n",
    "    \"historical_avg_temperature\",\n",
    "    \"temperature_anomaly\",\n",
    "    \"high_climate_risk\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de59051-387d-453c-9c70-c150b375dd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ml_df = ml_df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f238ed8e-94be-40fa-a602-45d2b966277b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "925acf88-590e-40f8-9714-466c49a651e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"year\",\n",
    "        \"avg_yearly_temperature\",\n",
    "        \"historical_avg_temperature\",\n",
    "        \"temperature_anomaly\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "train_data = assembler.transform(train_df)\n",
    "test_data = assembler.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f442c3e7-452d-471b-b546-e46e04457c5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"high_climate_risk\"\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dcf80e1-b586-4ae0-9ee7-77f8768086ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f90f296b-eb3c-4915-9994-27fd5535f103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9894846200095516"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"high_climate_risk\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f0dbe8-f1b2-429e-9458-6449e85c2e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----+\n|high_climate_risk|prediction|count|\n+-----------------+----------+-----+\n|                0|       1.0|  129|\n|                1|       0.0|  197|\n|                1|       1.0| 1066|\n|                0|       0.0| 7850|\n+-----------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy(\n",
    "    \"high_climate_risk\", \"prediction\"\n",
    ").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd038f5-4318-4196-87dd-e221335435cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DenseVector([0.0086, 0.1387, 0.0695, 11.3299])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b14289d3-e1c1-486c-aeae-f7f129139fa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/29 07:48:27 INFO mlflow.tracking.fluent: Experiment with name '/Shared/climate_risk_country_model' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/1930267809943715', creation_time=1769672907629, experiment_id='1930267809943715', last_update_time=1769672907629, lifecycle_stage='active', name='/Shared/climate_risk_country_model', tags={'mlflow.experiment.sourceName': '/Shared/climate_risk_country_model',\n",
       " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
       " 'mlflow.ownerEmail': 'abhayshinde6754@gmail.com',\n",
       " 'mlflow.ownerId': '73261402331391'}>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "mlflow.set_experiment(\"/Shared/climate_risk_country_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cefeb2ab-abea-40d0-928f-b863deeed3a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/workspace/default/climate_raw/mlflow_tmp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ea84f9-a415-47ec-8dc9-f78604dcf806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/29 07:52:09 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/01/29 07:52:13 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-d0091602-20e6-49d2-886b-ac/tmpsj5_j45i/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/01/29 07:52:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "mlflow.set_experiment(\"/Shared/climate_risk_country_model\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"high_climate_risk\"\n",
    "    )\n",
    "\n",
    "    lr_model = lr.fit(train_data)\n",
    "\n",
    "    predictions = lr_model.transform(test_data)\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"high_climate_risk\",\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "\n",
    "    mlflow.log_metric(\"auc\", auc)\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        lr_model,\n",
    "        artifact_path=\"logistic_regression_model\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a81a67-5f9f-421d-88fa-a0439bd904fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_country_df = spark.table(\"gold_climate_country_indicators\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b0f468-3f63-472a-87c7-240dba4fbd2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ml_df = gold_country_df.select(\n",
    "    \"year\",\n",
    "    \"avg_yearly_temperature\",\n",
    "    \"historical_avg_temperature\",\n",
    "    \"temperature_anomaly\",\n",
    "    \"high_climate_risk\"\n",
    ").dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c85139-5a32-4ef6-bfde-12694678dc28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c27e7b-0955-431e-a272-8bd8b545508f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"year\",\n",
    "        \"avg_yearly_temperature\",\n",
    "        \"historical_avg_temperature\",\n",
    "        \"temperature_anomaly\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "train_data = assembler.transform(train_df)\n",
    "test_data = assembler.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "522d00c4-7f37-4cb0-aa05-6d9f0c5187d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mMlflowException\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7647533210658436>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmlflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m model_uri \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels:/logistic_regression_model/latest\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m----> 4\u001B[0m lr_model \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mload_model(model_uri)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/spark/__init__.py:961\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(model_uri, dfs_tmpdir, dst_path)\u001B[0m\n",
       "\u001B[1;32m    957\u001B[0m \u001B[38;5;66;03m# This MUST be called prior to appending the model flavor to `model_uri` in order\u001B[39;00m\n",
       "\u001B[1;32m    958\u001B[0m \u001B[38;5;66;03m# for `artifact_path` to take on the correct value for model loading via mlflowdbfs.\u001B[39;00m\n",
       "\u001B[1;32m    959\u001B[0m root_uri, artifact_path \u001B[38;5;241m=\u001B[39m _get_root_uri_and_artifact_path(model_uri)\n",
       "\u001B[0;32m--> 961\u001B[0m local_mlflow_model_path \u001B[38;5;241m=\u001B[39m _download_artifact_from_uri(\n",
       "\u001B[1;32m    962\u001B[0m     artifact_uri\u001B[38;5;241m=\u001B[39mmodel_uri, output_path\u001B[38;5;241m=\u001B[39mdst_path\n",
       "\u001B[1;32m    963\u001B[0m )\n",
       "\u001B[1;32m    964\u001B[0m flavor_conf \u001B[38;5;241m=\u001B[39m Model\u001B[38;5;241m.\u001B[39mload(local_mlflow_model_path)\u001B[38;5;241m.\u001B[39mflavors[FLAVOR_NAME]\n",
       "\u001B[1;32m    965\u001B[0m _add_code_from_conf_to_system_path(local_mlflow_model_path, flavor_conf)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracking/artifact_utils.py:108\u001B[0m, in \u001B[0;36m_download_artifact_from_uri\u001B[0;34m(artifact_uri, output_path, lineage_header_info)\u001B[0m\n",
       "\u001B[1;32m    100\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    101\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n",
       "\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    artifact_uri: The *absolute* URI of the artifact to download.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m    lineage_header_info: The model lineage header info to be consumed by lineage services.\u001B[39;00m\n",
       "\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    107\u001B[0m root_uri, artifact_path \u001B[38;5;241m=\u001B[39m _get_root_uri_and_artifact_path(artifact_uri)\n",
       "\u001B[0;32m--> 108\u001B[0m repo \u001B[38;5;241m=\u001B[39m get_artifact_repository(artifact_uri\u001B[38;5;241m=\u001B[39mroot_uri)\n",
       "\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(repo, ModelsArtifactRepository):\n",
       "\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m repo\u001B[38;5;241m.\u001B[39mdownload_artifacts(\n",
       "\u001B[1;32m    112\u001B[0m         artifact_path\u001B[38;5;241m=\u001B[39martifact_path,\n",
       "\u001B[1;32m    113\u001B[0m         dst_path\u001B[38;5;241m=\u001B[39moutput_path,\n",
       "\u001B[1;32m    114\u001B[0m         lineage_header_info\u001B[38;5;241m=\u001B[39mlineage_header_info,\n",
       "\u001B[1;32m    115\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/artifact_repository_registry.py:133\u001B[0m, in \u001B[0;36mget_artifact_repository\u001B[0;34m(artifact_uri)\u001B[0m\n",
       "\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_artifact_repository\u001B[39m(artifact_uri: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ArtifactRepository:\n",
       "\u001B[1;32m    121\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    122\u001B[0m \u001B[38;5;124;03m    Get an artifact repository from the registry based on the scheme of artifact_uri\u001B[39;00m\n",
       "\u001B[1;32m    123\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    131\u001B[0m \u001B[38;5;124;03m        requirements.\u001B[39;00m\n",
       "\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _artifact_repository_registry\u001B[38;5;241m.\u001B[39mget_artifact_repository(artifact_uri)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/artifact_repository_registry.py:77\u001B[0m, in \u001B[0;36mArtifactRepositoryRegistry.get_artifact_repository\u001B[0;34m(self, artifact_uri)\u001B[0m\n",
       "\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m repository \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n",
       "\u001B[1;32m     74\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not find a registered artifact repository for: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00martifact_uri\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     75\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrently registered schemes are: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_registry\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     76\u001B[0m     )\n",
       "\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m repository(artifact_uri)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/models_artifact_repo.py:48\u001B[0m, in \u001B[0;36mModelsArtifactRepository.__init__\u001B[0;34m(self, artifact_uri)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m registry_uri \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mget_registry_uri()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_databricks_unity_catalog_uri(uri\u001B[38;5;241m=\u001B[39mregistry_uri):\n",
       "\u001B[0;32m---> 48\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo \u001B[38;5;241m=\u001B[39m UnityCatalogModelsArtifactRepository(\n",
       "\u001B[1;32m     49\u001B[0m         artifact_uri\u001B[38;5;241m=\u001B[39martifact_uri, registry_uri\u001B[38;5;241m=\u001B[39mregistry_uri\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo\u001B[38;5;241m.\u001B[39mmodel_name\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_version \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo\u001B[38;5;241m.\u001B[39mmodel_version\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/unity_catalog_models_artifact_repo.py:92\u001B[0m, in \u001B[0;36mUnityCatalogModelsArtifactRepository.__init__\u001B[0;34m(self, artifact_uri, registry_uri)\u001B[0m\n",
       "\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
       "\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
       "\u001B[0;32m---> 92\u001B[0m model_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_version \u001B[38;5;241m=\u001B[39m get_model_name_and_version(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient, artifact_uri)\n",
       "\u001B[1;32m     93\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name \u001B[38;5;241m=\u001B[39m get_full_name_from_sc(model_name, spark)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/utils/models.py:99\u001B[0m, in \u001B[0;36mget_model_name_and_version\u001B[0;34m(client, models_uri)\u001B[0m\n",
       "\u001B[1;32m     97\u001B[0m     mv \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget_model_version_by_alias(model_name, model_alias)\n",
       "\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_name, mv\u001B[38;5;241m.\u001B[39mversion\n",
       "\u001B[0;32m---> 99\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model_name, \u001B[38;5;28mstr\u001B[39m(_get_latest_model_version(client, model_name, model_stage))\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/utils/models.py:31\u001B[0m, in \u001B[0;36m_get_latest_model_version\u001B[0;34m(client, name, stage)\u001B[0m\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_latest_model_version\u001B[39m(client, name, stage):\n",
       "\u001B[1;32m     27\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;124;03m    Returns the latest version of the stage if stage is not None. Otherwise return the latest of all\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;124;03m    versions.\u001B[39;00m\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m---> 31\u001B[0m     latest \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget_latest_versions(name, \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m stage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m [stage])\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(latest) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m     33\u001B[0m         stage_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m and stage \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracking/_model_registry/client.py:161\u001B[0m, in \u001B[0;36mModelRegistryClient.get_latest_versions\u001B[0;34m(self, name, stages)\u001B[0m\n",
       "\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_latest_versions\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, stages\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n",
       "\u001B[1;32m    149\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Latest version models for each requests stage. If no ``stages`` provided, returns the\u001B[39;00m\n",
       "\u001B[1;32m    150\u001B[0m \u001B[38;5;124;03m    latest version for each stage.\u001B[39;00m\n",
       "\u001B[1;32m    151\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    159\u001B[0m \n",
       "\u001B[1;32m    160\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 161\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstore\u001B[38;5;241m.\u001B[39mget_latest_versions(name, stages)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/_unity_catalog/registry/rest_store.py:485\u001B[0m, in \u001B[0;36mUcModelRegistryStore.get_latest_versions\u001B[0;34m(self, name, stages)\u001B[0m\n",
       "\u001B[1;32m    471\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    472\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m    473\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDetected attempt to load latest model version in stages \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstages\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    474\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou may see this error because:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    482\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthat the version number is a valid integer\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    483\u001B[0m     )\n",
       "\u001B[0;32m--> 485\u001B[0m _raise_unsupported_method(\n",
       "\u001B[1;32m    486\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget_latest_versions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    487\u001B[0m     message\u001B[38;5;241m=\u001B[39mmessage,\n",
       "\u001B[1;32m    488\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/_unity_catalog/registry/rest_store.py:137\u001B[0m, in \u001B[0;36m_raise_unsupported_method\u001B[0;34m(method, message)\u001B[0m\n",
       "\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    136\u001B[0m     messages\u001B[38;5;241m.\u001B[39mappend(message)\n",
       "\u001B[0;32m--> 137\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(messages))\n",
       "\n",
       "\u001B[0;31mMlflowException\u001B[0m: Method 'get_latest_versions' is unsupported for models in the Unity Catalog. To load the latest version of a model in Unity Catalog, you can set an alias on the model version and load it by alias. See https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags for details."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "MlflowException",
        "evalue": "Method 'get_latest_versions' is unsupported for models in the Unity Catalog. To load the latest version of a model in Unity Catalog, you can set an alias on the model version and load it by alias. See https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags for details."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>MlflowException</span>: Method 'get_latest_versions' is unsupported for models in the Unity Catalog. To load the latest version of a model in Unity Catalog, you can set an alias on the model version and load it by alias. See https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags for details.\n[Trace ID: 00-f479e4baf04e71ec4dee9d588d6591c4-cceab2f670a7fa5d-00]"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mMlflowException\u001B[0m                           Traceback (most recent call last)",
        "File \u001B[0;32m<command-7647533210658436>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmlflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspark\u001B[39;00m\n\u001B[1;32m      3\u001B[0m model_uri \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels:/logistic_regression_model/latest\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 4\u001B[0m lr_model \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mload_model(model_uri)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/spark/__init__.py:961\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(model_uri, dfs_tmpdir, dst_path)\u001B[0m\n\u001B[1;32m    957\u001B[0m \u001B[38;5;66;03m# This MUST be called prior to appending the model flavor to `model_uri` in order\u001B[39;00m\n\u001B[1;32m    958\u001B[0m \u001B[38;5;66;03m# for `artifact_path` to take on the correct value for model loading via mlflowdbfs.\u001B[39;00m\n\u001B[1;32m    959\u001B[0m root_uri, artifact_path \u001B[38;5;241m=\u001B[39m _get_root_uri_and_artifact_path(model_uri)\n\u001B[0;32m--> 961\u001B[0m local_mlflow_model_path \u001B[38;5;241m=\u001B[39m _download_artifact_from_uri(\n\u001B[1;32m    962\u001B[0m     artifact_uri\u001B[38;5;241m=\u001B[39mmodel_uri, output_path\u001B[38;5;241m=\u001B[39mdst_path\n\u001B[1;32m    963\u001B[0m )\n\u001B[1;32m    964\u001B[0m flavor_conf \u001B[38;5;241m=\u001B[39m Model\u001B[38;5;241m.\u001B[39mload(local_mlflow_model_path)\u001B[38;5;241m.\u001B[39mflavors[FLAVOR_NAME]\n\u001B[1;32m    965\u001B[0m _add_code_from_conf_to_system_path(local_mlflow_model_path, flavor_conf)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracking/artifact_utils.py:108\u001B[0m, in \u001B[0;36m_download_artifact_from_uri\u001B[0;34m(artifact_uri, output_path, lineage_header_info)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    artifact_uri: The *absolute* URI of the artifact to download.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m    lineage_header_info: The model lineage header info to be consumed by lineage services.\u001B[39;00m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    107\u001B[0m root_uri, artifact_path \u001B[38;5;241m=\u001B[39m _get_root_uri_and_artifact_path(artifact_uri)\n\u001B[0;32m--> 108\u001B[0m repo \u001B[38;5;241m=\u001B[39m get_artifact_repository(artifact_uri\u001B[38;5;241m=\u001B[39mroot_uri)\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(repo, ModelsArtifactRepository):\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m repo\u001B[38;5;241m.\u001B[39mdownload_artifacts(\n\u001B[1;32m    112\u001B[0m         artifact_path\u001B[38;5;241m=\u001B[39martifact_path,\n\u001B[1;32m    113\u001B[0m         dst_path\u001B[38;5;241m=\u001B[39moutput_path,\n\u001B[1;32m    114\u001B[0m         lineage_header_info\u001B[38;5;241m=\u001B[39mlineage_header_info,\n\u001B[1;32m    115\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/artifact_repository_registry.py:133\u001B[0m, in \u001B[0;36mget_artifact_repository\u001B[0;34m(artifact_uri)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_artifact_repository\u001B[39m(artifact_uri: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ArtifactRepository:\n\u001B[1;32m    121\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;124;03m    Get an artifact repository from the registry based on the scheme of artifact_uri\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;124;03m        requirements.\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _artifact_repository_registry\u001B[38;5;241m.\u001B[39mget_artifact_repository(artifact_uri)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/artifact_repository_registry.py:77\u001B[0m, in \u001B[0;36mArtifactRepositoryRegistry.get_artifact_repository\u001B[0;34m(self, artifact_uri)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m repository \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\n\u001B[1;32m     74\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not find a registered artifact repository for: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00martifact_uri\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     75\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCurrently registered schemes are: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_registry\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     76\u001B[0m     )\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m repository(artifact_uri)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/models_artifact_repo.py:48\u001B[0m, in \u001B[0;36mModelsArtifactRepository.__init__\u001B[0;34m(self, artifact_uri)\u001B[0m\n\u001B[1;32m     46\u001B[0m registry_uri \u001B[38;5;241m=\u001B[39m mlflow\u001B[38;5;241m.\u001B[39mget_registry_uri()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_databricks_unity_catalog_uri(uri\u001B[38;5;241m=\u001B[39mregistry_uri):\n\u001B[0;32m---> 48\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo \u001B[38;5;241m=\u001B[39m UnityCatalogModelsArtifactRepository(\n\u001B[1;32m     49\u001B[0m         artifact_uri\u001B[38;5;241m=\u001B[39martifact_uri, registry_uri\u001B[38;5;241m=\u001B[39mregistry_uri\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo\u001B[38;5;241m.\u001B[39mmodel_name\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_version \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrepo\u001B[38;5;241m.\u001B[39mmodel_version\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/unity_catalog_models_artifact_repo.py:92\u001B[0m, in \u001B[0;36mUnityCatalogModelsArtifactRepository.__init__\u001B[0;34m(self, artifact_uri, registry_uri)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m---> 92\u001B[0m model_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_version \u001B[38;5;241m=\u001B[39m get_model_name_and_version(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient, artifact_uri)\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name \u001B[38;5;241m=\u001B[39m get_full_name_from_sc(model_name, spark)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/utils/models.py:99\u001B[0m, in \u001B[0;36mget_model_name_and_version\u001B[0;34m(client, models_uri)\u001B[0m\n\u001B[1;32m     97\u001B[0m     mv \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget_model_version_by_alias(model_name, model_alias)\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_name, mv\u001B[38;5;241m.\u001B[39mversion\n\u001B[0;32m---> 99\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model_name, \u001B[38;5;28mstr\u001B[39m(_get_latest_model_version(client, model_name, model_stage))\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/artifact/utils/models.py:31\u001B[0m, in \u001B[0;36m_get_latest_model_version\u001B[0;34m(client, name, stage)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_latest_model_version\u001B[39m(client, name, stage):\n\u001B[1;32m     27\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124;03m    Returns the latest version of the stage if stage is not None. Otherwise return the latest of all\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124;03m    versions.\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m     latest \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mget_latest_versions(name, \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m stage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m [stage])\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(latest) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     33\u001B[0m         stage_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stage \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m and stage \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/tracking/_model_registry/client.py:161\u001B[0m, in \u001B[0;36mModelRegistryClient.get_latest_versions\u001B[0;34m(self, name, stages)\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_latest_versions\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, stages\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    149\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Latest version models for each requests stage. If no ``stages`` provided, returns the\u001B[39;00m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;124;03m    latest version for each stage.\u001B[39;00m\n\u001B[1;32m    151\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    159\u001B[0m \n\u001B[1;32m    160\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 161\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstore\u001B[38;5;241m.\u001B[39mget_latest_versions(name, stages)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/_unity_catalog/registry/rest_store.py:485\u001B[0m, in \u001B[0;36mUcModelRegistryStore.get_latest_versions\u001B[0;34m(self, name, stages)\u001B[0m\n\u001B[1;32m    471\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    472\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    473\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDetected attempt to load latest model version in stages \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstages\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    474\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou may see this error because:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    482\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthat the version number is a valid integer\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    483\u001B[0m     )\n\u001B[0;32m--> 485\u001B[0m _raise_unsupported_method(\n\u001B[1;32m    486\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget_latest_versions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    487\u001B[0m     message\u001B[38;5;241m=\u001B[39mmessage,\n\u001B[1;32m    488\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/store/_unity_catalog/registry/rest_store.py:137\u001B[0m, in \u001B[0;36m_raise_unsupported_method\u001B[0;34m(method, message)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    136\u001B[0m     messages\u001B[38;5;241m.\u001B[39mappend(message)\n\u001B[0;32m--> 137\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m MlflowException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(messages))\n",
        "\u001B[0;31mMlflowException\u001B[0m: Method 'get_latest_versions' is unsupported for models in the Unity Catalog. To load the latest version of a model in Unity Catalog, you can set an alias on the model version and load it by alias. See https://mlflow.org/docs/latest/model-registry.html#deploy-and-organize-models-with-aliases-and-tags for details."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow.spark\n",
    "\n",
    "model_uri = \"models:/logistic_regression_model/latest\"\n",
    "lr_model = mlflow.spark.load_model(model_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe13bf9-7e58-436c-866d-14577dcce59b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"high_climate_risk\"\n",
    ")\n",
    "lr_model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd6b9fc7-c711-4515-bba2-2d88bee45568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fa7db6-6d78-4972-b10c-ac88b6955bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction_df = predictions.select(\n",
    "    \"year\",\n",
    "    \"avg_yearly_temperature\",\n",
    "    \"historical_avg_temperature\",\n",
    "    \"temperature_anomaly\",\n",
    "    \"high_climate_risk\",\n",
    "    \"prediction\",\n",
    "    \"probability\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf2ed30-8f92-4ef9-95de-2c54eb2d66e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    prediction_df\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(\"gold_country_climate_predictions\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8213efc-f357-4f1f-87c2-036b2ebb2945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------+--------------------------+-------------------+-----------------+----------+-----------+\n|year|avg_yearly_temperature|historical_avg_temperature|temperature_anomaly|high_climate_risk|prediction|probability|\n+----+----------------------+--------------------------+-------------------+-----------------+----------+-----------+\n|1743|1.3230000000000002    |4.611730053342408         |-3.288730053342408 |0                |0.0       |[1.0,0.0]  |\n|1743|2.4819999999999998    |6.1774884661218925        |-3.6954884661218927|0                |0.0       |[1.0,0.0]  |\n|1743|3.572                 |6.96089656395415          |-3.38889656395415  |0                |0.0       |[1.0,0.0]  |\n|1743|5.096                 |8.992271734195894         |-3.896271734195894 |0                |0.0       |[1.0,0.0]  |\n|1743|5.431                 |10.33786160764953         |-4.906861607649531 |0                |0.0       |[1.0,0.0]  |\n+----+----------------------+--------------------------+-------------------+-----------------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"SELECT * FROM gold_country_climate_predictions LIMIT 5\"\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3187bc2-f99a-41db-889e-e5e4a220d066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}